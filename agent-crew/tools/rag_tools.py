import json
import logging
import os
import warnings
import boto3
from typing import Type, List, Any
from crewai.tools import BaseTool
from pydantic import BaseModel, PrivateAttr
from langchain_chroma import Chroma
import chromadb
from langchain_huggingface import HuggingFaceEmbeddings
from schemas.tool_input import SearchInternalDocsInput, AdaptiveRagInput
from schemas.task_output import RagPlan
from openai import OpenAI

logger = logging.getLogger(__name__)

class SearchOrgChartTool(BaseTool):
    name: str = "ì¡°ì§ë„ ë° ì—…ë¬´ ë¶„ì¥í‘œ ë¡œë“œ ë„êµ¬"
    description: str = "ì—…ë¬´ë¶„ì¥í‘œ JSON ì „ì²´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."
    
    _minio_bucket: str = PrivateAttr()
    _minio_object_key: str = PrivateAttr()
    _s3_client = PrivateAttr(default=None)

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self._minio_bucket = os.getenv("MINIO_BUCKET", "academic-bucket")
        self._minio_object_key = os.getenv("ORG_CHART_JSON_KEY", "software_org_chart.json")
        try:
            self._s3_client = boto3.client(
                's3', endpoint_url=os.getenv("MINIO_ENDPOINT_URL", "http://minio:9000"),
                aws_access_key_id=os.getenv("MINIO_ACCESS_KEY", "ajou"),
                aws_secret_access_key=os.getenv("MINIO_SECRET_KEY", "software"), use_ssl=False
            )
        except:
            self._s3_client = None

    def _run(self) -> str:
        if self._s3_client is None: return "Error: MinIO client not initialized."
        try:
            response = self._s3_client.get_object(Bucket=self._minio_bucket, Key=self._minio_object_key)
            data = json.loads(response['Body'].read().decode('utf-8'))
            return json.dumps(data.get("ì—…ë¬´ë¶„ì¥í‘œ", []), indent=2, ensure_ascii=False)
        except Exception as e:
            return f"Error loading org chart: {e}"

class ListKnowledgeBaseFilesTool(BaseTool):
    name: str = "RAG íŒŒì¼ ëª©ë¡ ì¡°íšŒ"
    description: str = "DBì— ì €ì¥ëœ PDF íŒŒì¼ëª… ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."
    
    _client = PrivateAttr(default=None)
    _collection_name: str = PrivateAttr()

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        try:
            self._collection_name = os.getenv("CHROMA_COLLECTION_NAME", "academic_regulations")
            self._client = chromadb.HttpClient(
                host=os.getenv("CHROMA_HOST", "chromadb"), 
                port=int(os.getenv("CHROMA_PORT", 8000))
            )
        except:
            self._client = None

    def _run(self) -> List[str]:
        if not self._client: return []
        try:
            coll = self._client.get_collection(name=self._collection_name)
            metas = coll.get(include=["metadatas"])['metadatas']
            return sorted(list(set(m['source'] for m in metas if m and 'source' in m)))
        except:
            return []\

class SearchInternalDocsTool(BaseTool):
    name: str = "RAG ë‹¨ì¼ ê²€ìƒ‰ (ë¬¸ë§¥ í™•ì¥ í¬í•¨)"
    description: str = "íŠ¹ì • íŒŒì¼ì—ì„œ ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ë‚´ìš©ì„ ê²€ìƒ‰í•˜ê³ , ì „í›„ ë¬¸ë§¥ì„ í¬í•¨í•˜ì—¬ ìƒì„¸ ë‚´ìš©ì„ ë°˜í™˜í•©ë‹ˆë‹¤."
    args_schema: Type[BaseModel] = SearchInternalDocsInput
    
    _vectorstore: Chroma = PrivateAttr(default=None)
    _search_k: int = PrivateAttr(default=6) 
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self._search_k = int(os.getenv("VECTOR_DB_K", 6))
        
        try:
            embeddings = HuggingFaceEmbeddings(
                model_name=os.getenv("EMBEDDING_MODEL", "jhgan/ko-sroberta-multitask"),
                model_kwargs={'device': os.getenv("DEVICE_TYPE", "cpu")}
            )
            client = chromadb.HttpClient(
                host=os.getenv("CHROMA_HOST", "chromadb"), 
                port=int(os.getenv("CHROMA_PORT", 8000))
            )
            self._vectorstore = Chroma(
                client=client,
                collection_name=os.getenv("CHROMA_COLLECTION_NAME", "academic_regulations"),
                embedding_function=embeddings
            )
        except Exception as e:
            logger.error(f"RAG Init Failed: {e}")

    def _run(self, query: str, source_file: str) -> str:
        if not self._vectorstore: return "Error: DB Not Initialized"
        
        logger.info(f"[RAG Tool] Search: '{query}' in '{source_file}' (K={self._search_k})")
        
        try:
            with warnings.catch_warnings():
                warnings.filterwarnings("ignore", category=UserWarning)
                docs_with_scores = self._vectorstore.similarity_search_with_relevance_scores(
                    query, k=self._search_k, filter={"source": source_file}
                )
            
            if not docs_with_scores:
                return f"Info: '{source_file}'ì—ì„œ '{query}' ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            
            final_result = ""
            # ìƒìœ„ 3ê°œ ê²°ê³¼ì— ëŒ€í•´ì„œë§Œ ì•ë’¤ ë¬¸ë§¥ í™•ì¥ ìˆ˜í–‰
            top_docs = docs_with_scores[:3]
            
            for i, (doc, score) in enumerate(top_docs):
                meta = doc.metadata
                chunk_id = meta.get('chunk_id')
                context_block = ""
                
                if chunk_id is not None:
                    # (1) ì´ì „ ì²­í¬
                    prev_data = self._vectorstore.get(
                        where={"$and": [{"source": source_file}, {"chunk_id": chunk_id - 1}]},
                        include=["documents"]
                    )
                    if prev_data and prev_data.get('documents'):
                        context_block += f"[ì´ì „ ë¬¸ë§¥]\n{prev_data['documents'][0]}\n"

                    # (2) í˜„ì¬ ì²­í¬
                    context_block += f"[ê²€ìƒ‰ëœ ë‚´ìš© (Score: {score:.4f})]\n{doc.page_content}\n"
                    
                    # (3) ë‹¤ìŒ ì²­í¬
                    next_data = self._vectorstore.get(
                        where={"$and": [{"source": source_file}, {"chunk_id": chunk_id + 1}]},
                        include=["documents"]
                    )
                    if next_data and next_data.get('documents'):
                        context_block += f"[ë‹¤ìŒ ë¬¸ë§¥]\n{next_data['documents'][0]}\n"
                        
                    # (4) ë‹¤ë‹¤ìŒ ì²­í¬
                    next_data = self._vectorstore.get(
                        where={"$and": [{"source": source_file}, {"chunk_id": chunk_id + 2}]},
                        include=["documents"]
                    )
                    if next_data and next_data.get('documents'):
                        context_block += f"[ë‹¤ë‹¤ìŒ ë¬¸ë§¥]\n{next_data['documents'][0]}\n"
                else:
                    context_block += f"[ê²€ìƒ‰ëœ ë‚´ìš©]\n{doc.page_content}\n"
                
                final_result += f"\n=== [Result #{i+1}] ===\n{context_block}\n"

            return final_result

        except Exception as e:
            logger.error(f"[RAG Tool] Error: {e}", exc_info=True)
            return f"Search Error: {e}"

class AdaptiveRagSearchTool(BaseTool):
    name: str = "ì§€ëŠ¥í˜• ê·œì •ì§‘ í†µí•© ê²€ìƒ‰ ë„êµ¬"
    description: str = "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì…ë ¥ë°›ì•„ ìµœì ì˜ PDFë¥¼ ì°¾ì•„ ê²€ìƒ‰í•©ë‹ˆë‹¤."
    args_schema: Type[BaseModel] = AdaptiveRagInput
    
    _list_tool: ListKnowledgeBaseFilesTool = PrivateAttr()
    _search_tool: SearchInternalDocsTool = PrivateAttr()
    _openai_client: OpenAI = PrivateAttr()

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self._list_tool = ListKnowledgeBaseFilesTool()
        self._search_tool = SearchInternalDocsTool()
        self._openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    def _run(self, query: Any = None, **kwargs) -> str:
        # ì…ë ¥ íŒŒë¼ë¯¸í„° ë°©ì–´ ë¡œì§
        if not query:
            query = kwargs.get('description') or kwargs.get('input') or kwargs.get('user_query')
        if isinstance(query, dict):
            query = query.get('query') or query.get('description') or str(query)
        if not query and kwargs:
            for v in kwargs.values():
                if isinstance(v, str) and len(v) > 5:
                    query = v
                    break
        query = str(query) if query else "ë‚´ìš© ì—†ìŒ"
            
        files = self._list_tool._run()
        if not files: return "Error: ê²€ìƒ‰ ê°€ëŠ¥í•œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
        files_str = "\n".join(files)

        # í”„ë¡¬í”„íŠ¸ ì „ëµ: 'ê²€ìƒ‰ì–´ ìƒì„± ì›ì¹™'ì„ ì œì‹œí•˜ì—¬ ì–´ë–¤ ì£¼ì œê°€ ì™€ë„ ëŒ€ì‘ ê°€ëŠ¥í•˜ê²Œ í•¨
        prompt = f"""
        ë‹¹ì‹ ì€ ëŒ€í•™ í–‰ì • ë°ì´í„° ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ê°€ì¥ ì í•©í•œ PDF íŒŒì¼ 1ê°œë¥¼ ì„ íƒí•˜ê³ ,
        RAG ê²€ìƒ‰ì„ ìœ„í•œ '3ê°€ì§€ ê²€ìƒ‰ì–´'ë¥¼ ìƒì„±í•˜ì„¸ìš”.

        [ê²€ìƒ‰ì–´ ìƒì„± ì›ì¹™]
        1. **ì§ê´€ì  ê²€ìƒ‰ì–´**: ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì˜ˆ: 'ì¡¸ì—… ìš”ê±´', 'ì¥í•™ê¸ˆ ê¸°ì¤€').
        2. **êµ¬ì¡°ì  ê²€ìƒ‰ì–´**: ê·œì •ì§‘ì˜ íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ 'í‘œ', 'ë³„í‘œ', 'ì„¸ë¶€ ê¸°ì¤€', 'ì˜ˆì™¸ ì‚¬í•­', 'ìœ ì˜ì‚¬í•­' ë“±ì˜ ë‹¨ì–´ë¥¼ ì¡°í•© (ì˜ˆ: 'ì¡¸ì—… ì„¸ë¶€ ê¸°ì¤€', 'ì¥í•™ê¸ˆ ì§€ê¸‰ ì œí•œ ì˜ˆì™¸').
        3. **ì—°ê´€ ê²€ìƒ‰ì–´**: ì§ˆë¬¸ì˜ ë¬¸ë§¥ì„ íŒŒì•…í•˜ì—¬ í–‰ì •ì ìœ¼ë¡œ ì—°ê´€ëœ ê³µì‹ ìš©ì–´ë¥¼ ìœ ì¶” (ì˜ˆ: 'ì‹¬í™” ê³¼ì •', 'ìˆ˜í˜œ ìê²©', 'ì´ìˆ˜ êµ¬ë¶„').

        [ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì¼ ëª©ë¡]
        {files_str}
        """
        
        try:
            completion = self._openai_client.beta.chat.completions.parse(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": prompt},
                    {"role": "user", "content": query},
                ],
                response_format=RagPlan,
            )
            plan = completion.choices[0].message.parsed
            target_file = plan.target_filename
            queries = plan.search_queries
            
            logger.info(f"ğŸ“‚ [AdaptiveRAG] Target: {target_file}")
            logger.info(f"â“ [AdaptiveRAG] Queries: {queries}")
            
            if target_file not in files: target_file = files[0]

        except Exception as e:
            logger.error(f"Planning Failed: {e}")
            target_file = files[0] if files else ""
            queries = [query]

        aggregated_results = f"--- [ê²€ìƒ‰ ëŒ€ìƒ: {target_file}] ---\n"
        for q in queries:
            search_res = self._search_tool._run(query=q, source_file=target_file)
            aggregated_results += f"\n[Q: {q}]\n{search_res}\n"
            
        return aggregated_results